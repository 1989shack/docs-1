---
title: Disaster Recovery from LiteFS Cloud
layout: docs
sitemap: false
nav: litefs
---

The biggest advantage of using LiteFS Cloud is that you have backups
to restore from in case of an unfortunate situation where you lose
all the nodes in your cluster. In this doc, we'll show you how to restore
your cluster from LiteFS Cloud if you've lost all the nodes.

<section class="warning icon">
You should only follow the steps in this document if you've lost all
nodes of your LiteFS cluster. Otherwise, you can
[restore from a LiteFS Cloud backup](/docs/litefs/cloud-restore).
</section>

## Start your app up with only the primary node

It's simpler to recover a single node, rather than several nodes, so to
start out with, it's best to remove all replica nodes and run just a
primary node. If your app hasn't already restarted before you got here,
just restart it with only the primary node and move on to [the next step](#find-the-correct-cluster-id)!

If your app has already restarted with multiple nodes running, then you should temporarily
remove all of the replicas.

**For apps on the Fly Platform:**

1. List the machines running your app:

    ```cmd
    fly status
    ```

    You'll get output that shows the Machines and their Roles. For example:

    ```
    ...
    Machines
    PROCESS	ID            	VERSION	REGION	STATE  	ROLE   	CHECKS	LAST UPDATED
    app    	784e900c2de378	1      	ams   	started	primary	      	2023-07-10T13:05:40Z
    app    	e784e471b39208	1      	ord   	started	replica	      	2023-07-10T13:01:16Z
    ...
    ```

2. Destroy each machine that's marked `replica`. For example:

    ```cmd
    fly machines destroy e784e471b39208
    ```

3. Run `fly status` again to confirm that you only have the primary running.

## Find the correct cluster id

When your app restarted, LiteFS generated a new cluster id, which is different
than the cluster id stored in LiteFS Cloud. This is a safety measure: we don't want
to accidentally overwrite data in a cluster if you inadvertently set the wrong LiteFS
Cloud token for the app. You'll see error messages in the LiteFS logs, similar
to the following:

```
level=INFO msg="backup stream failed, retrying: fetch position map: backup client error (422): cluster id mismatch, expecting \"LFSC25AD000F32ED02A9\""
```

Note the cluster id specified at the end of the log message: `LFSC25AD000F32ED02A9` in the example log. Copy the cluster id from your own logs to use later.

**For apps on the Fly Platform:**

View the logs with:

  ```cmd
  fly logs
  ```

## Update the cluster id

Now that you have the correct cluster id, you can connect to your primary node and
update the cluster id file. After you update the cluster id, restart LiteFS.

Check your logs again, and confirm you that don't see the cluster id error message anymore!

You should also take a look at your app and confirm that you see the data you expect
before you continue.

**For apps on the Fly Platform:**

1. Connect to the primary node:

    ```cmd
    fly ssh console
    ```

1. Update the `clusterid` file. For example:

    ```sh
    echo LFSC25AD000F32ED02A9 > /var/lib/litefs/clusterid
    ```

1. Close your ssh session:
    ```sh
    <machine ID>:/myapp# exit
    ```

1. Restart the app:

    ```cmd
    fly app restart
    ```

1. Check the logs to make sure the cluster id error is gone, and make sure your app is working the way you expect it to.

## Resolve consul key error

For apps on the Fly Platform (or you're using Consul for lease management
elsewhere), you may see the consul errors in your logs. For example:

```
2023-07-10T13:30:25Z app[784e900c2de378] ams [info]level=INFO msg="cannot connect, \"consul\" lease already initialized with different ID: LFSCA5E3680C1C2FCFFE"
```

You have two options to resolve the Consul key error:

- Change the Consul key value that LiteFS uses. This is easy, but leaves extra Consul keys set.

**or**

- Remove the "wrong" key from Consul. This is more difficult, but cleaner.

### Easy option: change the Consul key

You can simply update the `lease.consul.key` value in the `litefs.yml` file. The old one will
still exist in Consul, but LiteFS won't care. For example, you can add `-v2` to the end
of the Consul key value:

```yml
lease:
  type: consul
  ...
  consul:
    url: "${FLY_CONSUL_URL}"
    key: "litefs/${FLY_APP_NAME}-v2" # added '-v2'
```

**For apps on the Fly Platform:**

After updating the `litefs.yml` file, redeploy your app:

```cmd
fly deploy
```

### Cleaner option: remove the wrong key from Consul

First, you'll need to add `consul` in your image. Connect to your primary node via ssh, and
install it there. Here's how to install it:

```sh
# For debian/ubuntu-based images
apt-get install consul

# For alpine-based images
apk add consul
```

Get your `FLY_CONSUL_URL`:

```sh
echo $FLY_CONSUL_URL
```

This url is in the format `https://:{TOKEN}@{HOST}/{PREFIX}`. You'll need each of these values
separately. You'll also need the value of `lease.consul.key` from your `litefs.yml` file.

Then use the [Consul CLI][] to delete the key with:

```sh
export TOKEN=(copy the token you found from your consul url)
export HOST=(copy the host you found from your consul url)
export PREFIX=(copy the prefix you found from your consul url)
export LITEFS_CONSUL_KEY=(copy the key from your litefs.yml file)

# and then delete the key!
consul kv delete -http-addr=https://$HOST -token=$TOKEN $PREFIX/$LITEFS_CONSUL_KEY/clusterid
```

[Consul CLI]: https://developer.hashicorp.com/consul/commands/kv

After this is done, restart LiteFS. 

**For apps on the Fly Platform:**

After removing the wrong key from Consul, restart your app:

```sh
fly app restart
```

## Add your replicas back

Once your primary node is connected to LiteFS Cloud again and your data has
been recovered, you should go ahead and scale back up.

**For apps on the Fly Platform:**

Scale back up by running something like this (replacing `ord`
with the region you'd like to run your new replica in):

```sh
fly machines clone --select --region ord
```